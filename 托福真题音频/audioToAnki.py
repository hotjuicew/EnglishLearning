from faster_whisper import WhisperModel
import subprocess
import pandas as pd
import os
from difflib import SequenceMatcher

# ===================== 1. è¯»å–å®Œæ•´å¬åŠ›æ–‡æœ¬ =====================
def load_full_text(text_file):
    print("ğŸ“„ è¯»å–å®Œæ•´å¬åŠ›æ–‡æœ¬...")
    english_sentences = []
    chinese_sentences = []

    with open(text_file, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f.readlines() if line.strip()]

    for i in range(0, len(lines), 2):  # æ¯ä¸¤è¡Œæ˜¯ä¸€ç»„ï¼ˆè‹±æ–‡ + ä¸­æ–‡ï¼‰
        if i + 1 < len(lines):
            english_sentences.append(lines[i])
            chinese_sentences.append(lines[i + 1])

    print(f"âœ… åŠ è½½å®Œæˆï¼šè‹±æ–‡ {len(english_sentences)} å¥ï¼Œä¸­æ–‡ {len(chinese_sentences)} å¥")
    return english_sentences, chinese_sentences

# ===================== 2. è¿è¡Œ Whisper è·å–é€è¯æ—¶é—´æˆ³ =====================
def transcribe_audio(audio_file):
    print("ğŸ” è¿è¡Œ Faster-Whisper è¯†åˆ«å®Œæ•´éŸ³é¢‘ï¼ˆè·å–æ¯ä¸ªå•è¯çš„æ—¶é—´æˆ³ï¼‰...")
    model = WhisperModel("medium", compute_type="int8")

    segments, _ = model.transcribe(audio_file, vad_filter=True)  # ä½¿ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹
    whisper_sentences = []
    timestamps = []

    for segment in segments:
        whisper_sentences.append(segment.text.strip())
        timestamps.append((segment.start, segment.end))

    print(f"âœ… Whisper è¯†åˆ«å®Œæˆï¼Œç”Ÿæˆ {len(whisper_sentences)} ä¸ªç‰‡æ®µ")
    return whisper_sentences, timestamps

# ===================== 3. ç”¨ `full_text.txt` å¯¹é½ Whisper è¯†åˆ«çš„æ—¶é—´æˆ³ =====================
def align_sentences(whisper_sentences, whisper_timestamps, full_text_sentences, whisper_results):
    print("ğŸ”„ æ­£åœ¨åŒ¹é… `full_text.txt` å¥å­...")
    aligned_timestamps = []

    for i, full_text in enumerate(full_text_sentences):
        best_match_index = max(
            range(len(whisper_sentences)),
            key=lambda j: SequenceMatcher(None, whisper_sentences[j], full_text).ratio()
        )

        # è®¡ç®—è¯¥å¥å­çš„å•è¯æ—¶é—´æˆ³
        words = whisper_results["segments"][best_match_index]["words"]
        if words:
            start_time = words[0]["start"]
            end_time = words[-1]["end"]
        else:
            start_time, end_time = whisper_timestamps[best_match_index]

        # è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œé¿å…æ—¶é—´æˆ³å‡ºç°åœ¨ä¸Šä¸€å¥æˆ–ä¸‹ä¸€å¥
        if i > 0:
            prev_end = aligned_timestamps[i - 1][1]
            start_time = max(start_time, prev_end + 0.1)  # é¿å…æ—¶é—´é‡å 
        if i < len(full_text_sentences) - 1:
            next_start = whisper_timestamps[min(best_match_index + 1, len(whisper_timestamps) - 1)][0]
            end_time = min(end_time, next_start - 0.1)

        aligned_timestamps.append((start_time, end_time))

    print(f"âœ… å¯¹é½å®Œæˆï¼Œæ‰€æœ‰ `full_text.txt` å¥å­å·²åŒ¹é…å¯¹åº”æ—¶é—´æˆ³")
    return aligned_timestamps

# ===================== 4. ä½¿ç”¨ FFmpeg è£å‰ªéŸ³é¢‘ =====================
def split_audio(audio_file, timestamps, output_folder="audio_clips"):
    print("âœ‚ï¸  å¼€å§‹å‰ªè¾‘éŸ³é¢‘...")
    os.makedirs(output_folder, exist_ok=True)
    audio_files = []

    for i, (start, end) in enumerate(timestamps):
        output_file = f"{output_folder}/sentence_{i+1}.mp3"
        command = f'ffmpeg -i "{audio_file}" -ss {start} -to {end} -q:a 0 -map a "{output_file}" -y'
        subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        audio_files.append(output_file)
        print(f"ğŸ§ å‰ªè¾‘å®Œæˆï¼š{output_file}")

    print(f"âœ… æ‰€æœ‰éŸ³é¢‘å‰ªè¾‘å®Œæˆï¼Œå…± {len(audio_files)} å¥")
    return audio_files

# ===================== 5. ç”Ÿæˆ Anki CSV æ–‡ä»¶ =====================
def create_anki_csv(english_sentences, chinese_sentences, audio_files, output_csv="anki_listening_deck.csv"):
    print("ğŸ“„  ç”Ÿæˆ Anki CSV æ–‡ä»¶...")
    
    data = [
        ("[sound:" + os.path.basename(audio) + "]", english + "<br><br>" + chinese)
        for english, chinese, audio in zip(english_sentences, chinese_sentences, audio_files)
    ]

    df = pd.DataFrame(data, columns=["Front", "Back"])
    df.to_csv(output_csv, index=False, encoding="utf-8-sig")

    print(f"âœ… Anki CSV æ–‡ä»¶å·²ç”Ÿæˆï¼š{output_csv}")
    return output_csv

# ===================== 6. ä¸»ç¨‹åºæ‰§è¡Œ =====================
def main():
    audio_file = "æ‰˜ç¦100çœŸé¢˜3L2.mp3"
    text_file = "full_text.txt"
    output_folder = "audio_clips"

    # è¯»å–å®Œæ•´æ–‡æœ¬
    english_sentences, chinese_sentences = load_full_text(text_file)

    # è¿è¡Œ Whisperï¼Œè·å–å®Œæ•´éŸ³é¢‘ + é€è¯æ—¶é—´æˆ³
    whisper_sentences, whisper_timestamps, whisper_results = transcribe_audio(audio_file)

    # è®© `whisper_timestamps` å¯¹é½ `full_text.txt`
    aligned_timestamps = align_sentences(whisper_sentences, whisper_timestamps, english_sentences, whisper_results)

    # è£å‰ªéŸ³é¢‘
    audio_files = split_audio(audio_file, aligned_timestamps, output_folder)

    # ç”Ÿæˆ Anki CSV
    csv_file = create_anki_csv(english_sentences, chinese_sentences, audio_files)

    print("\nğŸš€ **æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼è¯·å°†ä»¥ä¸‹æ–‡ä»¶å¯¼å…¥ Ankiï¼š**")
    print(f"ğŸ“‚ {csv_file} ï¼ˆå¡ç‰‡æ•°æ®ï¼‰")
    print(f"ğŸ“‚ {output_folder}/  ï¼ˆæ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ï¼Œéœ€æ”¾å…¥ Anki media ç›®å½•ï¼‰")

if __name__ == "__main__":
    main()
